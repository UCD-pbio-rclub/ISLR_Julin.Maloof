---
title: "Nov 29"
author: "Julin N Maloof"
date: "11/29/2017"
output: 
  html_document: 
    keep_md: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## 2.2

_Explain whether each scenario is a classification or regression prob- lem, and indicate whether we are most interested in inference or pre- diction. Finally, provide n and p._

_(a) We collect a set of data on the top 500 firms in the US. For each firm we record profit, number of employees, industry and the CEO salary. We are interested in understanding which factors affect CEO salary._

This is a regression problem with n = salary and p = profit, employees, and industry.  We are interested in inference.

_(b) We are considering launching a new product and wish to know whether it will be a success or a failure. We collect data on 20 similar products that were previously launched. For each prod- uct we have recorded whether it was a success or failure, price charged for the product, marketing budget, competition price, and ten other variables._

This is a classification problem, with n = successs/failure and p = everything else.  We are interested in prediction.

_(c) We are interested in predicting the % change in the USD/Euro exchange rate in relation to the weekly changes in the world stock markets. Hence we collect weekly data for all of 2012. For each week we record the % change in the USD/Euro, the % change in the US market, the % change in the British market, and the % change in the German market._

This is a regression problem, with n = % change in exchange rate and p = everything else.  We are interested in prediction.

## 2.7

```{r}
library(tidyverse)
data <- tibble( # A tibble is like a data frame, but better.  See http://r4ds.had.co.nz/
  X1= c(0,2,0,0,-1,1),
  X2= c(3,0,1,1,0,1),
  X3 = c(0,0,3,2,1,1),
  Y = c("red","red","red","green","green","red"))
data
```

Euclidian distance is sqrt of the sum of the squared distances.

```{r}
newpoint <- c(0,0,0)
data %>% # a pipe.  See the pipes chapter at http://r4ds.had.co.nz/
  select(-Y) %>%
  apply(1,function(x) {(newpoint - x)^2 %>% sum() %>% sqrt()})
```

Or we could use the distance fuction
```{r}
data %>% 
  select(-Y) %>%
  bind_rows(c(X1=0,X2=0,X3=0)) %>%
  dist() # we are interested in the last row
```

_(b) What is our prediction with K = 1? Why?_

For K=1 we take the nearest neighbor (observation 5).  So, "Green":

```{r}
data[5,]
```


_(c) What is our prediction with K = 3? Why?_

For K=3 would would take the 3 closest points and figure out their majority answer ("red"):

```{r}
data[c(2,5,6),]
```

_(d) If the Bayes decision boundary in this problem is highly non- linear, then would we expect the best value for K to be large or small? Why?_

Small....becuase small K will capture the subtlies in the boundary better, where as large K will average over them.

```{r, eval=FALSE}
install.packages("ISLR")
```

_9. This exercise involves the Auto data set studied in the lab. Make sure that the missing values have been removed from the data._
_(a) Which of the predictors are quantitative, and which are quali- tative?_

```{r}
library(ISLR)
data(Auto)
auto <- Auto %>% na.omit() %>% as.tibble()
auto
```

qualitative: name, origin, possibly year and possibly cylinder

_(b) What is the range of each quantitative predictor? You can an- swer this using the range() function._

```{r}
auto %>% select(-name) %>% map(range) # apply a function to every colum, or every item in a list.
auto %>% select(-name) %>% apply( 2, range) # apply a function over rows(1) or columns(2) 
```


_(c) What is the mean and standard deviation of each quantitative predictor?_

```{r}
auto %>% select(-name) %>% apply(2,mean)
cat("---------\n")
auto %>% select(-name) %>% apply(2,sd)
```


(d) Now remove the 10th through 85th observations. What is the range, mean, and standard deviation of each predictor in the subset of the data that remains?

```{r}
auto_subset <- auto[-10:-85,] %>% select(-name)

cat("range\n")
auto_subset %>% apply(2,range)
cat("\nmean\n")
auto_subset %>% apply(2,mean)
cat("\nsd\n")
auto_subset %>% apply(2,sd)
```


(e) Using the full data set, investigate the predictors graphically, using scatterplots or other tools of your choice. Create some plots highlighting the relationships among the predictors. Comment on your findings.

```{r}
auto %>% select(-name) %>% pairs()

auto %>% ggplot(aes(x=cylinders,y=mpg)) +
  geom_point() +
  geom_smooth(method = "lm")

auto %>% ggplot(aes(x=origin,y=mpg)) +
  geom_point() +
  geom_smooth(method = "lm")

auto %>% ggplot(aes(x=acceleration,y=mpg)) +
  geom_point() +
  geom_smooth(method="lm")
```

_(f) Suppose that we wish to predict gas mileage (mpg) on the basis of the other variables. Do your plots suggest that any of the other variables might be useful in predicting mpg? Justify your answer._

All could be predictive; accelaration is probably the least useful (most scatter).
Displacement, horespower, and weight are all highly correlated so it may not be helpful/necessary to include them all.

_10. This exercise involves the Boston housing data set._

_(a) To begin, load in the Boston data set. The Boston data set is
part of the MASS library in R. > library(MASS)
Now the data set is contained in the object Boston. > Boston
Read about the data set:
?Boston_

```{r}
library(MASS)
data(Boston)
boston <- as_tibble(Boston)
?Boston
summary(boston)
```


_How many rows are in this data set? How many columns? What do the rows and columns represent?_

506 rows (1 per town); 14 columns (variables)


_(b) Make some pairwise scatterplots of the predictors (columns) in this data set. Describe your findings._

```{r}
pairs(boston) #uggh.  Too many varaibles to see
```

_(c) Are any of the predictors associated with per capita crime rate? If so, explain the relationship._

scatter plot for each predicitor vs. crime.
```{r, warning=FALSE}
plots <- lapply(colnames(boston)[-1], function(p) { #creates a list of plots
  ggplot(boston, aes_string(x=p,y="crim")) +
    geom_point(shape=1) +
    geom_smooth(method="loess") + 
    ggtitle(p)
})

sapply(plots, print) # now for each item in the list, print it 
```

Because the predictors are mostly highly non-normal it is a bit difficult to see relationships here.

(d) Do any of the suburbs of Boston appear to have particularly high crime rates? Tax rates? Pupil-teacher ratios? Comment on the range of each predictor.

```{r}
boston %>% gather(key="variable", value="value", everything()) %>% #gather take wide format and converts to long
  ggplot(aes(x=variable,y=value)) +
  geom_violin() +
  facet_wrap(~variable, scales="free",nrow=2)

boston %>% gather(key="variable", value="value", everything()) %>%
  ggplot(aes(x=variable,y=value)) +
  geom_boxplot() +
  facet_wrap(~variable, scales="free",nrow=2)
```



(e) How many of the suburbs in this data set bound the Charles river?

```{r}
sum(boston$chas)
```

(f) What is the median pupil-teacher ratio among the towns in this data set?

```{r}
median(boston$ptratio)
```

(g) Which suburb of Boston has lowest median value of owner- occupied homes? What are the values of the other predictors for that suburb, and how do those values compare to the overall ranges for those predictors? Comment on your findings.

```{r}
(boston_min_medv <- boston[which.min(boston$medv),])
```

```{r}
boston_min_medv_gather <- boston_min_medv %>% gather(key="variable", value="value", everything())
boston %>% gather(key="variable", value="value", everything()) %>%
  ggplot(aes(x=variable,y=value)) +
  geom_boxplot() +
  geom_hline(data=boston_min_medv_gather,aes(yintercept=value),color="red") +
  facet_wrap(~variable, scales="free",nrow=2)
```

The suburub with the lowest median value has the oldest houses, high crime, low distnace to employtment center, high "lstat", high nox, high ptratio, high rad, high tax, and low zoning as large lots.


(h) In this data set, how many of the suburbs average more than seven rooms per dwelling? More than eight rooms per dwelling? Comment on the suburbs that average more than eight rooms per dwelling.





